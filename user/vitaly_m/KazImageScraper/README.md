# Google Image Scraper

A CLI tool that automates Google Images scraping using Selenium and Chromedriver.

The repository also includes a supporting script for unfolding those images based on CSV.

### Pre-requisites
1. Python 3.6+
2. Google Chrome Browser (matching version of the Chromedriver).
3. Chromedriver installed and accessible via PATH.
4. Python3 packages from requirements.txt

### Installation
1. Clone the repository.
2. Install required Python packages:
```bash
pip install -r requirements.txt
```
3. Ensure that chromedriver is properly installed.


### Usage: Main CLI (GoogleImageScraper)
This is the primary script that scrapes images from Google.
#### Command-Line Arguments:
* --search-keys: Space-separated keywords to search for (e.g., --search-keys cat dog apple). 
* --search-keys-file: Path to a JSON file containing search requests (an array of objects). Each object should have a "keywords" list. 
* --images: Number of images to scrape per keyword (default: 30). 
* --headless: Run browser in headless mode (no GUI). 
* --min-resolution: Minimum image resolution (width height) (default: 400 400). 
* --max-resolution: Maximum image resolution (width height) (default: 8000 8000). 
* --max-missed: Maximum times a click can fail before stopping (default: 200). 
* --workers: Number of parallel threads to use (default: 1). 
* --webdriver-dir: Directory containing chromedriver (default: webdriver). 
* --photos-dir: Directory to save images (default: photos).

#### Example Usage of Main CLI
```bash
 python GoogleImageScraper.py \
  --search-keys-file Json_example.json \
  --images 20 --headless --min-resolution 300 300 \
  --max-resolution 3000 3000 \
  --max-missed 30 \
  --workers 2
```
After running, each keyword will have its own folder in photos/KEYWORD, containing the downloaded images and a metadata.json file with image info.

If necessary, use flag --webdriver-dir to specify chromedriver path:
```bash
python GoogleImageScraper.py ... --webdriver-dir "/path/to/chromedriver_folder/chromedriver(.exe)"
```

### Usage: unfolding_crutch.py
This supporting script helps organize your downloaded folders (and their images) into a hierarchy, based on CSV metadata.

#### How it works
1. Reads a CSV file (path specified in .env) containing columns:
* ```work_cat_name```
* ```work_subcat_name```
* ```keywords (comma-separated list)```
2. For each row, it creates a folder structure under DST_ROOT/work_cat_name/work_subcat_name.
3. It moves each folder from SRC_ROOT/keyword â†’ DST_ROOT/work_cat_name/work_subcat_name/keyword.

This reads environment variables from .env and performs the re-organization, please check ```.env.example```
#### Running it
```bash
python unfolding_crutch.py
```

### Notes
* Make sure the CSV includes the required columns (work_cat_name, work_subcat_name, keywords). 
* The keywords column must contain comma-separated strings exactly matching the folder names generated by the main CLI. 
* Ensure that your chromedriver version matches your installed Chrome browser; if you see mismatch errors, update one of them. 

### Additional useful tool: chromedriver update
```bash
chmod +x ./chromedriver_install.sh

./chromedriver_install.sh
```
